---
title: "Homework 2 - Ali Lakdalwala, Kunle Lawal, Anu Rana, Mihir Tulpule"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction to Analytical Modeling
```{r, eval= TRUE, message= TRUE, echo = TRUE}

# Loading and Reading the data 
data = read.table("credit_card_data.txt")

library(kernlab)
library(kknn)
library(data.table)

head(data)
```
## Question 3.1 - Cross Validation (a)

```{r}
# Using k nearest neighbor we perform cross validation with kcv as 20. 
kcvarray = list(10, 20, 30, 40, 50, 60)
for (array in kcvarray) {

modelcv <- cv.kknn(V11 ~ ., data = data, kcv = 20)
head(modelcv[[1]])

# Exploring the results from the model
cv = data.table(modelcv[[1]])
cv$yhat <- ifelse(cv$yhat < 0.49, 0, 1)
# Printing model accuracy:
print('Cross Validation Accuracy:'); print(array)
print(table(cv$y == cv$yhat))
print(prop.table(table(cv$y == cv$yhat)))
print(mean(cv$y == cv$yhat))


}
```

kcv set to 10 gives us the best classifer. The model accuracy decreases if we increase the number of folds past 50.

&nbsp;

&nbsp;

## Question 3.1 - Training, Validation, Test Datasets (b)

### Dataset Creation
```{r}
# Sampling the data to create a training set 
Sample <- sample(1:654, (654*0.6))
data.train <- data[Sample, ]

# Creating a subset of the data for validation and test. 
data.notrain <- data[-Sample, ]

# Sampling from the subset 
Sample2 <- sample(1:197, (197*0.5))
# Validaton data set
data.validation <- (data.notrain[Sample2, ])

# Test data set
data.test <- data.notrain[-Sample2, ]
```
### Creating a model w/ Training Dataset
```{r}
# Trying different values of k to find the best classifier. 
kmaxarray = list(11, 31, 71)

for(array in kmaxarray){

model <- train.kknn(V11 ~ ., data = data.train, kmax = array)
model

prediction <- predict(model, data.validation[, -11])
prediction
# Factoring the prediction
t_prediction <- ifelse(prediction < 0.49, 0, 1)

CM <- table(data.validation[, 11], t_prediction)
CM

# Printing accuracy of model
accuracy <- (sum(diag(CM)))/sum(CM)
print(array)
accuracy
print(accuracy)

}

```

### Retraning the model on the combined dataset 

```{r}

combineddata = rbind(data.train, data.validation)
model <- train.kknn(V11 ~ ., data = combineddata, kmax = 61)
model
# Testing the model
prediction <- predict(model, data.test[, -11])
prediction

t_prediction <- ifelse(prediction < 0.49, 0, 1)

CM <- table(data.test[, 11], t_prediction)
CM
# Final Accuracy
accuracy <- (sum(diag(CM)))/sum(CM)
print(array)
accuracy

```
We get a high final accuracy ~ >85% from our model by splitting the data in  training, validation and test data proving we have found a good classifier. 

\newpage

## Question 4.1

*Describe a situation or problem from your job, everyday life, current events, etc., for which a clustering model would be appropriate. List some (up to 5) predictors that you might use.*

One real-life situation for which a clustering model would be appropriate would be a retail business using k-means clustering to determine the optimal number of delivery routes/launch locations. Ventures such as Amazon with large numbers of warehouses often attempt to maximize their efficiency and profits by optimizing the locations of their distribution centers, making such a use of k-means clustering vital to the growth of their operations. Five predictors that could be used to create such an optimization via clustering include:

1.	Population density (e.g. by district/metropolitan area)
2.	Amount of businesses in the defined area (tied with economic status of average resident in area)
3.	Most frequently ordered product categories in area
4.	Distance from nearest existing distribution center(s)
5.	Demographics of surrounding population

Using such predictors, among other fluctuating variables (e.g. weather in the area, etc.), Amazon is able to determine efficient routes that will minimize the amount of expenses/time spent in making frequent deliveries and thereby maximize their profits. As a result, we can see that k-means clustering has real-life applications with tangible impacts upon even entrepreneurial behemoths like Amazon.com.


## Question 4.2
In this question we use a for loop to find the best combination of predictors. 
```{r}
iris = read.table("iris.txt")

library(ggplot2)
# Loading the data
head(iris)

# Looping through every single combination. 
for(m in list(1, 2, 3, 4)){
  
  for(l in combn(list(1, 2, 3, 4), m, simplify = FALSE)){
    
    fit <- kmeans(iris[,unlist(l)], centers = 3, nstart=20)
    fit
    fit$cluster
    print(unlist(l))
    #Finding classification errors. 
    print(table(fit$cluster, iris$Species))
    
  }
  

}

```

From the results we can see that `Petal.Width` and `Petal.Width, Sepal.Width` are the best predictors with both resulting in only 6 misclassifications.  


